{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes Mixture Model\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Gaussian Mixture Model is great but text doesn't follow Gaussian distribution. We need another algorithm for text clustering. If you recall generative learning algorithms, we have Naïve Bayes apart from Gaussian Discriminant Analysis. It should not come as a surprise that we have Naïve Bayes Mixture Model as well. Mathematically, it is somewhat similar to Naïve Bayes. In Naïve Bayes, we can predict a new data point based upon Bayes' rule where p(y=label|x)=p(x|y=label)\\*p(y=label)/p(x). The concept holds in NBMM. To make our life easier, the following script only takes the case of multivariate, to be more precise, a binary cluster problem. I will leave you guys to explore multinomial NBMM.\n",
    "\n",
    "Like GMM, p(y) remains a latent variable we do not observe. In order to estimate p(y), we need to take an initial guess and gradually converge to the optimal number throughout iterations. Undoubtedly, we use Expectation-Maximization algorithm as well.\n",
    "\n",
    "The material from Stanford university doesn't really cover the details of NBMM. It has been briefly introduced in Andrew Ng's lecture video. If you fully comprehend the concept of Naïve Bayes and Gaussian Mixture Model, the equations can be easily derived. The only issue is the lack of literature on how to select the optimal number of clusters in NBMM. Since it's distribution-based, I assume you can use AIC and BIC as GMM. I will leave you guys to explore.\n",
    "\n",
    "Reference to Gaussian Mixture Model\n",
    "\n",
    "https://github.com/je-suis-tm/machine-learning/blob/master/gaussian%20mixture%20model.ipynb\n",
    "\n",
    "Reference to Naïve Bayes\n",
    "\n",
    "https://github.com/je-suis-tm/machine-learning/blob/master/naive%20bayes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk.tokenize\n",
    "import nltk.corpus\n",
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('K:/ecole/github')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text into a list of words\n",
    "#we can use stemming and lemmatization to improve efficiency\n",
    "#for instance, we have words walked,walking,walks\n",
    "#with nltk package, we can revert all of them to walk\n",
    "def text2list(text,stopword,lower=True,\n",
    "              lemma=False,stemma=False):\n",
    "\n",
    "    text_clean=text if lower==False else text.lower()\n",
    "    \n",
    "    #tokenize and remove stop words\n",
    "    token=[i for i in nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text_clean) if i not in stopword]\n",
    "    \n",
    "    #lemmatization\n",
    "    if lemma:\n",
    "        text_processed=[nltk.stem.wordnet.WordNetLemmatizer().lemmatize(i) for i in token]\n",
    "    else:\n",
    "        text_processed=token\n",
    "        \n",
    "    #stemming\n",
    "    if stemma:\n",
    "        output=[nltk.stem.PorterStemmer().stem(i) for i in text_processed]\n",
    "    else:\n",
    "        output=text_processed\n",
    "    \n",
    "    #remove numbers as they are stopword as well\n",
    "    for i in [ii for ii in output]:\n",
    "        try:\n",
    "            float(i)\n",
    "            output.remove(i)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return [i for i in output if i not in stopword]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E Step\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        w^{(i)}:=\\frac {P(X^{(i)}|Z_1) \\times P(Z_1)} {\\sum_{k=0}^{1} {P(X^{(i)}|Z_k) \\times P(Z_k)}}=\\frac{\\prod_{j=0}^{n} {P(x_j^{(i)}|Z_1)} \\times P(Z_1)}\n",
    "        {\\prod_{j=0}^{n}{P(x_j^{(i)}|Z_1)} \\times P(Z_1) + \\prod_{j=0}^{n} {P(x_j^{(i)}|Z_0)} \\times P(Z_0)}\n",
    "    \\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e step\n",
    "#compute the posterior probability with given params\n",
    "def e_step(matrix,params):\n",
    "\n",
    "    posterior=[[],[]]\n",
    "\n",
    "    #unpacking\n",
    "    prob_x_given_z,prob_z_0,prob_z_1=params\n",
    "\n",
    "    #transpose the matrix\n",
    "    transpose=matrix.T\n",
    "    transpose.drop('PRIOR',inplace=True)\n",
    "\n",
    "    #iterate through all datasets\n",
    "    for i in transpose:\n",
    "        phi_x_given_z_0=1.0\n",
    "        phi_x_given_z_1=1.0\n",
    "\n",
    "        #chain rules of independent events\n",
    "        for j in transpose[transpose[i]==1].index:\n",
    "            phi_x_given_z_0*=prob_x_given_z[j][0]\n",
    "            phi_x_given_z_1*=prob_x_given_z[j][1]\n",
    "\n",
    "        #when the probability gets too small\n",
    "        #python gives zero\n",
    "        #we d rather have ham email rather than spam email\n",
    "        #fear of missing out\n",
    "        if phi_x_given_z_0==0 or phi_x_given_z_1==0:\n",
    "            phi_z_0=1\n",
    "            phi_z_1=0\n",
    "        else:\n",
    "            phi_z_0=phi_x_given_z_0*prob_z_0/(phi_x_given_z_0*prob_z_0+phi_x_given_z_1*prob_z_1)\n",
    "            phi_z_1=phi_x_given_z_1*prob_z_1/(phi_x_given_z_0*prob_z_0+phi_x_given_z_1*prob_z_1)\n",
    "\n",
    "        posterior[0].append(phi_z_0)\n",
    "        posterior[1].append(phi_z_1)\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### M Step\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    P(x_j|Z_1):=\\frac{\\sum_{i=0}^{m} {w^{(i)} \\times 1\\{x_j^{(i)}=1\\}}} {\\sum_{i=0}^{m}{w^{(i)}}}    \n",
    "    \\end{split}\n",
    "    \\\\\n",
    "    \\begin{split}\n",
    "    P(x_j|Z_0):=\\frac{\\sum_{i=0}^{m} {(1-w^{(i)}) \\times 1\\{x_j^{(i)}=1\\}}} {\\sum_{i=0}^{m} {(1-w^{(i)})}}\n",
    "    \\end{split}\n",
    "    \\\\\n",
    "    \\begin{split}\n",
    "    P(Z_1):=\\frac{\\sum_{i=0}^{m}{w^{(i)}}}{m}\n",
    "    \\end{split}\n",
    "    \\\\\n",
    "    \\begin{split}\n",
    "    P(Z_0):=\\frac{\\sum_{i=0}^{m} {(1-w^{(i)})}}{m}\n",
    "    \\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m step\n",
    "#use posterior probability to update params\n",
    "def m_step(matrix):\n",
    "\n",
    "    #compute P(x_j|Z)\n",
    "    prob_x_given_z=pd.DataFrame(columns=matrix.columns)\n",
    "    for j in matrix.columns:\n",
    "\n",
    "        #compute the frequency of word j coinciding with the given cluster\n",
    "        freq_j_given_z_1=len(matrix[matrix[j]==1][matrix['PRIOR']==1])\n",
    "        freq_j_given_z_0=len(matrix[matrix[j]==1][matrix['PRIOR']==0])\n",
    "\n",
    "        #laplace smoothing\n",
    "        if freq_j_given_z_1==0:\n",
    "            prob_x_given_z_1=1/(len(matrix)+2)\n",
    "        else:\n",
    "            prob_x_given_z_1=freq_j_given_z_1/len(matrix)\n",
    "        if freq_j_given_z_0==0:\n",
    "            prob_x_given_z_0=1/(len(matrix)+2)\n",
    "        else:\n",
    "            prob_x_given_z_0=freq_j_given_z_0/len(matrix)\n",
    "\n",
    "        #create dataframe\n",
    "        prob_x_given_z[j]=[prob_x_given_z_0,prob_x_given_z_1]\n",
    "\n",
    "    #compute P(Z)\n",
    "    prob_z_1=matrix['PRIOR'].sum()/len(matrix)\n",
    "    prob_z_0=1-prob_z_1\n",
    "\n",
    "    return (prob_x_given_z,prob_z_0,prob_z_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lower Bound\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    l(\\theta) \\ge \\sum_{i=0}^{m} { \\sum_{k=0}^{1} {w^{(i)} \\times \\log \\frac {P(X^{(i)}|Z_k) \\times P(Z_k)} {w^{(i)}} } } = \\sum_{i=0}^{m} { \\sum_{k=0}^{1} {w^{(i)} \\times (\\log P(X^{(i)}|Z_k) + \\log P(Z_k) - \\log {w^{(i)}}) } }\n",
    "    \\end{split}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using jensens inequality to compute the lower bound\n",
    "#which is the function of the expectation\n",
    "def get_lower_bound(matrix,posterior,params):\n",
    "\n",
    "    phi_z_0=[];phi_z_1=[]\n",
    "    \n",
    "    #catch logarithm zero issue\n",
    "    np.seterr(divide='raise')\n",
    "\n",
    "    #unpacking\n",
    "    prob_x_given_z,prob_z_0,prob_z_1=params\n",
    "\n",
    "    #transpose the matrix\n",
    "    transpose=matrix.T\n",
    "    transpose.drop('PRIOR',inplace=True)\n",
    "\n",
    "    #iterate through all datasets\n",
    "    for i in transpose:\n",
    "        phi_x_given_z_0=1.0\n",
    "        phi_x_given_z_1=1.0\n",
    "\n",
    "        #chain rules of independent events\n",
    "        for j in transpose[transpose[i]==1].index:\n",
    "            phi_x_given_z_0*=prob_x_given_z[j][0]\n",
    "            phi_x_given_z_1*=prob_x_given_z[j][1]\n",
    "\n",
    "        #when the probability gets too small\n",
    "        #python gives zero\n",
    "        #logarithm does not take zero\n",
    "        #to avoid that, use 1 instead\n",
    "        #so no effect on lower bound\n",
    "        if phi_x_given_z_0==0:\n",
    "            phi_x_given_z_0=1\n",
    "        if phi_x_given_z_1==0:\n",
    "            phi_x_given_z_1=1\n",
    "\n",
    "        phi_z_0.append(phi_x_given_z_0)\n",
    "        phi_z_1.append(phi_x_given_z_1)\n",
    "\n",
    "    #logarithm does not take zero\n",
    "    #to avoid that, use 1 instead\n",
    "    #so no effect on lower bound\n",
    "    for i in range(2):\n",
    "        for ind,val in enumerate(posterior[i]):\n",
    "            if val==0:\n",
    "                posterior[i][ind]=1\n",
    "\n",
    "    #sum up\n",
    "    summation_0=posterior[0]*(np.log(phi_z_0)+np.log(prob_z_0)-np.log(posterior[0]))\n",
    "    summation_1=posterior[1]*(np.log(phi_z_1)+np.log(prob_z_1)-np.log(posterior[1]))\n",
    "\n",
    "    return sum(summation_0+summation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using mle for training\n",
    "def training(matrix,tolerance=0.001,num_of_itr=50,diagnosis=False):\n",
    "    \n",
    "    #initialize\n",
    "    lower_bound_old=None\n",
    "    lower_bound=None\n",
    "    counter=0        \n",
    "    \n",
    "    #use m step to get params\n",
    "    params=m_step(matrix)\n",
    "\n",
    "    #cap the maximum number of iterations\n",
    "    while counter<num_of_itr:        \n",
    "                                            \n",
    "        #e step\n",
    "        posterior=e_step(matrix,params)\n",
    "            \n",
    "        #update prior\n",
    "        matrix['PRIOR']=np.where(np.array(posterior[0])>np.array(posterior[1]),0,1)\n",
    "\n",
    "        #m step\n",
    "        params=m_step(matrix)\n",
    "        \n",
    "        #use lower bound to determine if converged\n",
    "        lower_bound_old=lower_bound\n",
    "        lower_bound=get_lower_bound(matrix,posterior,params)\n",
    "        if lower_bound_old and np.abs(lower_bound/lower_bound_old-1)<tolerance:\n",
    "            if diagnosis:\n",
    "                print(f'{counter} iterations to reach convergence\\n')\n",
    "            return params\n",
    "                        \n",
    "        counter+=1\n",
    "           \n",
    "    if diagnosis:\n",
    "        print(f'{counter} iterations to reach convergence\\n')\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the main function\n",
    "def nbmm(df,tolerance=0.001,num_of_itr=50,diagnosis=False):\n",
    "    \n",
    "    data=df.copy()\n",
    "    data['y']=data['spam']\n",
    "    \n",
    "    #tokenization\n",
    "    tokens=[]\n",
    "    for i in data['text'].tolist():\n",
    "        tokens.append(text2list(i,stopword,\n",
    "                                lower=True,lemma=True))            \n",
    "    data['word']=tokens\n",
    "\n",
    "    #get vocabulary\n",
    "    vocabulary=set([j for i in data['word'] for j in i if i not in stopword])\n",
    "\n",
    "    #using dataframe to create vocabulary matrix\n",
    "    #it is slow but more convenient\n",
    "    matrix=pd.DataFrame(columns=vocabulary)\n",
    "    for i in vocabulary:\n",
    "        temp=[]\n",
    "        for j in range(len(data)):\n",
    "            if i in data['word'][j]:\n",
    "                temp.append(1)  \n",
    "            else:\n",
    "                temp.append(0)\n",
    "        matrix[i]=temp\n",
    "\n",
    "    #initial parameters\n",
    "    #use random prior probability\n",
    "    prior=[np.random.randint(low=0,high=2) for i in range(len(df))]\n",
    "    matrix['PRIOR']=prior\n",
    "    \n",
    "    #obtain parameters of nbmm\n",
    "    params=training(matrix,tolerance,num_of_itr,diagnosis)\n",
    "    \n",
    "    #compute the prediction probability\n",
    "    posterior=e_step(matrix, params)\n",
    "    \n",
    "    #make forecast based upon probability\n",
    "    data['label']=np.where(np.array(posterior[0])>np.array(posterior[1]),0,1)\n",
    "    \n",
    "    #compute accuracy\n",
    "    erreur=0\n",
    "    checked=[]\n",
    "    for i in data['y'].unique():\n",
    "        erreur+=get_accuracy(data,i,checked)\n",
    "        checked.append(i)\n",
    "    accuracy=1-erreur/len(df)\n",
    "    if diagnosis:\n",
    "        print('\\naccuracy: %s'%(accuracy))\n",
    "    \n",
    "    return params,posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for unsupervised learning, clf.score doesnt return the accuracy\n",
    "#there is no cross validation, no known labels\n",
    "#the only way to detect the accuracy is vote of the majority\n",
    "#for each label given\n",
    "#we check which iris type is the majority\n",
    "#we consider the majority as the correct classification\n",
    "#all we need to do is to count the minority\n",
    "def get_accuracy(data,class_,checked):\n",
    "    \n",
    "    df=data.copy()\n",
    "    \n",
    "    #use dictionary to keep track of everything\n",
    "    d={}\n",
    "    \n",
    "    #counting\n",
    "    for i in df['label'][df['y']==class_].unique():\n",
    "        if i not in checked and i!=-1:\n",
    "            d[i]=df['label'][df['y']==class_].tolist().count(i)\n",
    "\n",
    "    #comparison\n",
    "    temp=-1\n",
    "    lbl=None\n",
    "    for i in d:\n",
    "        if d[i]>temp:\n",
    "            lbl=i\n",
    "            temp=d[i]\n",
    "\n",
    "    return len(df['label'][df['y']==class_][df['label']!=lbl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword=nltk.corpus.stopwords.words('english')+['u',\n",
    " 'beyond',\n",
    " 'within',\n",
    " 'around',\n",
    " 'would',\n",
    " 'b',\n",
    " 'c',\n",
    " 'e',\n",
    " 'f',\n",
    " 'g',\n",
    " 'h',\n",
    " 'j',\n",
    " 'k',\n",
    " 'l',\n",
    " 'n',\n",
    " 'p',\n",
    " 'q',\n",
    " 'r',\n",
    " 'u',\n",
    " 'v',\n",
    " 'w',\n",
    " 'x',\n",
    " 'z',\n",
    " 'first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the raw data really comes from my email\n",
    "df=pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 iterations to reach convergence\n",
      "\n",
      "\n",
      "accuracy: 0.5833333333333333\n"
     ]
    }
   ],
   "source": [
    "params,posterior=nbmm(df,diagnosis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
