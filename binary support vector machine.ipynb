{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Support Vector Machine\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "SVM is where we really step into the door of machine learning. I personally think Andrew Ng's lectures did not explain very well on this topic. I strongly advise readers to use alternative materials or the website SVM tutorial. This site was created by a Pole developer called Alexandre Kowalczyk. He dedicated to teach SVM from the very basic notation of vectors and LaGrangian to the hardcore Wolfe dual problem and SMO algorithm. It is very friendly for beginners who forget everything about high school math, haha. The free e-book he wrote is a must-read for more advanced techniques such as L1/L2 regularized soft margin, kernels, SMO and multiclass classification.\n",
    "\n",
    "Link to this awesome website\n",
    "\n",
    "https://www.svm-tutorial.com/\n",
    "\n",
    "For multiclass classification\n",
    "\n",
    "https://github.com/je-suis-tm/machine-learning/blob/master/multiclass%20support%20vector%20machine.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt.solvers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "os.chdir('d:/python/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using official sklearn package with the same parameters\n",
    "def skl_binary_svm(x_train,x_test,y_train,y_test,**kwargs):\n",
    "    \n",
    "    m=SVC(**kwargs).fit(np.array(x_train).reshape(-1, 1), \\\n",
    "                        np.array(y_train).ravel())\n",
    "    \n",
    "    train=m.score(np.array(x_train).reshape(-1, 1), \\\n",
    "                  np.array(y_train).ravel())*100\n",
    "    test=m.score(np.array(x_test).reshape(-1, 1), \\\n",
    "                 np.array(y_test).ravel())*100\n",
    "    \n",
    "    print('\\ntrain accuracy: %s'%(train)+'%')\n",
    "    print('\\ntest accuracy: %s'%(test)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm for binary classification\n",
    "def binary_svm(x_train,x_test,y_train,y_test,\n",
    "               kernel='linear',poly_constant=0.0,poly_power=1,gamma=5):\n",
    "\n",
    "    #this is outer product matrix\n",
    "    #which is the combination of all inner products\n",
    "    #alternatively,we can write outer product in\n",
    "    #np.mat([np.dot(y_train[i],y_train[j]) \n",
    "    #for j in y_train.index for i in y_train.index]).reshape(\n",
    "    #len(y_train),len(y_train))\n",
    "    #or just np.mat(y_train).T*np.mat(y_train)\n",
    "    y_product=np.outer(y_train,y_train)\n",
    "    \n",
    "    #using different kernels to map inner product to a higher dimension space\n",
    "    #there are only three kernels here, which are linear, polynomial, gaussian\n",
    "    if kernel=='linear':\n",
    "        x_product=np.outer(x_train,x_train)\n",
    "    elif kernel=='polynomial':\n",
    "        arr=np.outer(x_train,x_train)\n",
    "        x_product=np.apply_along_axis(\n",
    "            lambda x:(x+poly_constant)**poly_power,\n",
    "            0,arr.ravel()).reshape(arr.shape)\n",
    "    else:\n",
    "        #gaussian/rbf kernel\n",
    "        #map to infinite dimension space\n",
    "        #be careful with the value of gamma\n",
    "        #when gamma is too large, it could be overfitted\n",
    "        #when gamma is too small, it could be underfitted\n",
    "        #better to use gridsearch to find an optimal gamma\n",
    "        arr=np.mat(\n",
    "            [i-j for j in x_train for i in x_train]).reshape(\n",
    "            len(x_train),len(x_train))\n",
    "        x_product=np.apply_along_axis(\n",
    "            lambda x:np.exp(-1*gamma*(np.linalg.norm(x))**2),\n",
    "            0,arr.ravel()).reshape(arr.shape)\n",
    "    \n",
    "    #plz refer to the following link\n",
    "    #for how to solve wolfe dual problem in cvxopt\n",
    "    # http://cvxopt.org/userguide/coneprog.html#quadratic-programming\n",
    "    P=cvxopt.matrix(x_product*y_product)\n",
    "    q=cvxopt.matrix(-1*np.ones(len(x_train)))\n",
    "    G=cvxopt.matrix(np.diag(-1 * np.ones(len(x_train))))\n",
    "    h=cvxopt.matrix(np.zeros(len(x_train)))\n",
    "    A=cvxopt.matrix(y_train,(1,len(x_train)))\n",
    "    b=cvxopt.matrix(0.0)\n",
    "\n",
    "    solution=cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    alpha=pd.Series(solution['x'])\n",
    "    w=np.sum(alpha*y_train*x_train)\n",
    "\n",
    "    #here i am using prof andrew ng's method of calculating b\n",
    "    #alternatively, we can do a normal average of all value b\n",
    "    #b=np.mean(y_train-w*x_train)\n",
    "    b=-(min(x_train[y_train==1.0]*w)+max(x_train[y_train==-1.0]*w))/2\n",
    "\n",
    "    print('\\ntrain accuracy: %s'%(len(\n",
    "        y_train[np.sign(\n",
    "            np.multiply(w,x_train)+b)==y_train])/len(y_train)*100)+'%')\n",
    "    print('\\ntest accuracy: %s'%(len(\n",
    "        y_test[np.sign(np.multiply(w,x_test)+b)==y_test])/len(y_test)*100)+'%')\n",
    "    print('\\nparameters w: %s'%(w))\n",
    "    print('\\nparameters b: %s'%(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the classification has to be float instead of int\n",
    "#this is requested by cvxopt\n",
    "#for a binary classification\n",
    "#the value should be either -1.0 or 1.0\n",
    "df['y']=np.select([df['type']=='Iris-setosa', \\\n",
    "                   df['type']=='Iris-versicolor', \\\n",
    "                   df['type']=='Iris-virginica'],[-1.0,0.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity, let us make it a binary classification\n",
    "df=df[df['y']!=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity, let us reduce the dimension of x to 1\n",
    "#reference to pca\n",
    "# https://github.com/je-suis-tm/machine-learning/blob/master/principal%20component%20analysis.ipynb\n",
    "high_dims=pd.concat([df[i] for i in df.columns if 'length' in i or 'width' in i],axis=1)\n",
    "x=PCA(n_components=1).fit_transform(high_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.Series([x[i].item() for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crucial!!!!\n",
    "#or we would get errors in the next step\n",
    "x_test.reset_index(inplace=True,drop=True)\n",
    "y_test.reset_index(inplace=True,drop=True)\n",
    "x_train.reset_index(inplace=True,drop=True)\n",
    "y_train.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.4761e+00 -6.0406e+00  2e+02  1e+01  2e+00\n",
      " 1: -1.3648e+00 -8.9590e-01  2e+01  1e+00  2e-01\n",
      " 2:  1.5920e-02 -6.2513e-01  6e-01  1e-15  2e-15\n",
      " 3: -1.7897e-01 -2.7387e-01  9e-02  2e-16  7e-16\n",
      " 4: -2.0894e-01 -2.8781e-01  8e-02  2e-16  5e-16\n",
      " 5: -2.6661e-01 -2.6956e-01  3e-03  2e-16  5e-16\n",
      " 6: -2.6887e-01 -2.6900e-01  1e-04  3e-16  4e-16\n",
      " 7: -2.6899e-01 -2.6899e-01  1e-06  1e-16  4e-16\n",
      " 8: -2.6899e-01 -2.6899e-01  1e-08  3e-16  3e-16\n",
      "Optimal solution found.\n",
      "\n",
      "train accuracy: 100.0%\n",
      "\n",
      "test accuracy: 100.0%\n",
      "\n",
      "parameters w: 0.7334706947134487\n",
      "\n",
      "parameters b: 0.41906322704309396\n"
     ]
    }
   ],
   "source": [
    "binary_svm(x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train accuracy: 100.0%\n",
      "\n",
      "test accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "skl_binary_svm(x_train,x_test,y_train,y_test,kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
