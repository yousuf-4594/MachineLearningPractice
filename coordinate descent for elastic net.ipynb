{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Elastic Net is essentially a fancy name for a regularized least squares with both L1 norm and L2 norm penalties. Regularized least squares can tackle the multicollinearity problem to certain degree in the setting of linear regression. Usually regularized least squares with L1 penalty is called Lasso Regression and regularized least squares with L2 penalty is called Ridge Regression. Elastic Net is attempting to have it both ways. \n",
    "\n",
    "Assuming we have an equation $y=x\\theta+\\epsilon$, where $y,\\epsilon \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^{m \\times n}$, $I \\in \\mathbb{R}^{n \\times n}$ and $\\theta \\in \\mathbb{R}^{n}$.\n",
    "\n",
    "* Ordinary Least Squares solves the problem by minimizing $J(\\theta)=(y-x\\theta)^T(y-x\\theta)$ with respect to $\\theta$. By setting the partial derivative $\\frac{\\partial J(\\theta)}{\\partial \\theta}=0$, we shall obtain $\\theta=(x^Tx)^{-1}x^Ty$. \n",
    "\n",
    "\n",
    "* Ridge Regression solves the problem by minimizing $J(\\theta)=(y-x\\theta)^T(y-x\\theta)+\\lambda\\theta^T\\theta$ with respect to $\\theta$. By setting the partial derivative $\\frac{\\partial J(\\theta)}{\\partial \\theta}=0$, we shall obtain $\\theta=(x^Tx+\\lambda I)^{-1}x^Ty$.\n",
    "\n",
    "\n",
    "* Lasso Regression solves the problem by minimizing $J(\\theta)=(y-x\\theta)^T(y-x\\theta)+\\lambda|\\theta|$ with respect to $\\theta$. By setting the partial derivative $\\frac{\\partial J(\\theta)}{\\partial \\theta}=0$, we shall obtain $\\theta=(x^Tx)^{-1}(x^Ty-\\frac {\\lambda} {2} sign(\\theta))$. Nah, that's wrong, the problem is L1 norm is not differentiable. The proper way to do it is via coordinate descent (Friedman, Hastie and Tibshirani, 2008).\n",
    "\n",
    "\n",
    "* Elastic Net Regression solves the problem by minimizing $J(\\theta)=(y-x\\theta)^T(y-x\\theta)+\\lambda_1\\theta^T\\theta+\\lambda_2|\\theta|$ with respect to $\\theta$. Same as Lasso, we cannot solve the equation in closed form. In some cases (particularly sklearn), the Lagrangian form of loss function is written as $J(\\theta)=(y-x\\theta)^T(y-x\\theta)+\\lambda [\\alpha\\theta^T\\theta+(1-\\alpha)|\\theta|]$ whereas $\\lambda=\\lambda_1+\\lambda_2$ and $\\alpha=\\frac {\\lambda_1} {\\lambda_1+\\lambda_2}$. The raison d'Ãªtre of this compound form is to treat L1 and L2 as a single constraint. When $\\lambda=0$, the equation reverts to OLS. \n",
    "\n",
    "More details of how to implement Lasso can be found in the link below.\n",
    "\n",
    "https://xavierbourretsicotte.github.io/lasso_implementation.html\n",
    "\n",
    "More details of how to implement Elastic Net can be found in the link below.\n",
    "\n",
    "https://towardsdatascience.com/regularized-linear-regression-models-dcf5aa662ab9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Descent\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Coordinate descent can be thought as an alternative gradient descent to solve constrained optimization. At each iteration, the algorithm selects a feature $\\theta_j$, which can be cyclic from the default order of features or stochastic from picking randomly. Then the algorithm computes partial residual $r_{i,j}$ without $\\theta_j$ and updates $\\theta_j$ with $r_{i,j}$.\n",
    "\n",
    "$$ r_{i,j} = y_i - \\sum_{k \\neq j} x_{i, k}\\theta_k$$\n",
    "\n",
    "$$\\theta_j^* = \\frac{1}{n}\\sum_{i=1}^{n} x_{i, j}r_{i, j}$$\n",
    "\n",
    "Check the original paper of coordinate descent on regularized least squares.\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2929880/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "import sklearn.model_selection\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple ols in linear regression\n",
    "def ols(X,Y):\n",
    "    return np.linalg.inv(X.T@X)@X.T@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression\n",
    "def ridge(X,Y,lambda_=0.5):\n",
    "    I=np.ones((X.shape[1],X.shape[1]))\n",
    "    return np.linalg.inv(X.T@X+lambda_*I)@X.T@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso regression\n",
    "def lasso(X,Y,lambda_=0.5,max_itr=1000,tol=0.01,\n",
    "         diagnosis=False):\n",
    "    \n",
    "    #normalize\n",
    "    #as l1 norm depends on the variable\n",
    "    #we need to remove the scale of each variable to be robust\n",
    "    X_norm=X/np.linalg.norm(X,axis=0)\n",
    "    \n",
    "    #initialize with one\n",
    "    theta=np.ones((X_norm.shape[1],1))\n",
    "    \n",
    "    #coordinate descent\n",
    "    stop=False\n",
    "    counter=0\n",
    "    while not stop:\n",
    "        theta_prev=theta.copy()\n",
    "        counter+=1\n",
    "        \n",
    "        #cyclic coordinate descent\n",
    "        for i in range(X_norm.shape[1]):\n",
    "            \n",
    "            #coordinate descent keeps one feature fixed\n",
    "            theta[i]=0.0\n",
    "            \n",
    "            #no need to do partial residual on intercept\n",
    "            if i==0:\n",
    "                theta[0]=(Y-X_norm@theta).mean()\n",
    "                continue\n",
    "                \n",
    "            #partial residual                \n",
    "            ols_partial_residual=X_norm[:,i]@(Y-X_norm@theta)    \n",
    "\n",
    "            #soft thresholding\n",
    "            theta[i]=np.sign(ols_partial_residual)*max(\n",
    "                abs(ols_partial_residual)-lambda_,0)/X.shape[0]\n",
    "        \n",
    "        #convergence check\n",
    "        if np.all(abs(theta/theta_prev-1)<tol):\n",
    "            if diagnosis:\n",
    "                print(f'Converged after {counter} iterations')\n",
    "            stop=True\n",
    "        \n",
    "        #maximum iteration check\n",
    "        if counter==max_itr:\n",
    "            stop=True\n",
    "            \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elastic net regression\n",
    "#note that alpha has different definitions in sklearn\n",
    "#which is equivalent to lambda_ in this function\n",
    "#our alpha is also different from the original paper \n",
    "#which is equivalent to (1-alpha) in the paper\n",
    "def elastic_net(X,Y,lambda_=0.5,alpha=0.5,\n",
    "                max_itr=1000,tol=0.01,diagnosis=False):\n",
    "    \n",
    "    #normalize\n",
    "    #as l1 norm depends on the variable\n",
    "    #we need to remove the scale of each variable to be robust\n",
    "    X_norm=X/np.linalg.norm(X,axis=0)\n",
    "    \n",
    "    #initialize with one\n",
    "    theta=np.ones((X_norm.shape[1],1))\n",
    "    \n",
    "    #coordinate descent\n",
    "    stop=False\n",
    "    counter=0\n",
    "    while not stop:\n",
    "        theta_prev=theta.copy()\n",
    "        counter+=1\n",
    "        \n",
    "        #cyclic coordinate descent\n",
    "        for i in range(X_norm.shape[1]):\n",
    "            \n",
    "            #coordinate descent keeps one feature fixed\n",
    "            theta[i]=0.0\n",
    "            \n",
    "            #no need to do partial residual on intercept\n",
    "            if i==0:\n",
    "                theta[0]=(Y-X_norm@theta).mean()\n",
    "                continue\n",
    "                \n",
    "            #partial residual                \n",
    "            ols_partial_residual=X_norm[:,i]@(Y-X_norm@theta)    \n",
    "\n",
    "            #soft thresholding\n",
    "            theta[i]=np.sign(\n",
    "                ols_partial_residual)*max(\n",
    "                abs(ols_partial_residual)-lambda_*(\n",
    "                    1-alpha),0)/(1+lambda_*alpha)/X.shape[0]\n",
    "        \n",
    "        #convergence check\n",
    "        if np.all(abs(theta/theta_prev-1)<tol):\n",
    "            if diagnosis:\n",
    "                print(f'Converged after {counter} iterations')\n",
    "            stop=True\n",
    "        \n",
    "        #maximum iteration check\n",
    "        if counter==max_itr:\n",
    "            stop=True\n",
    "            \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris dataset is the default choice in this repository\n",
    "#plz refer to the website in the following link for original data\n",
    "# https://archive.ics.uci.edu/ml/datasets/iris\n",
    "iris=sklearn.datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make life easier\n",
    "#we start with a binary classification problem\n",
    "dataset=iris.data[iris.target!=2]\n",
    "Y=iris.target[iris.target!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use principal component analysis to reduce dimensions for viz\n",
    "#check link below for details on pca\n",
    "# https://github.com/je-suis-tm/machine-learning/blob/master/principal%20component%20analysis.ipynb\n",
    "X=sklearn.decomposition.PCA(n_components=2).fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure its in the vector form\n",
    "Y=Y.reshape(-1,1)\n",
    "\n",
    "#adding intercept\n",
    "#you can also do \n",
    "#import statsmodels.api as sm\n",
    "#X=sm.add_constant(X\n",
    "constant=np.ones((X.shape[0],1))\n",
    "X=np.concatenate([constant,X],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Comparison\n",
      "[[ 0.5         0.29194508 -0.1693774 ]] [[ 0.5       ]\n",
      " [ 0.29194508]\n",
      " [-0.1693774 ]]\n"
     ]
    }
   ],
   "source": [
    "#ols\n",
    "#our linear algebra yields the same result as sklearn\n",
    "clf=sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "clf.fit(X,Y)\n",
    "\n",
    "theta_ols_skl=clf.coef_\n",
    "theta_ols_self=ols(X,Y)\n",
    "\n",
    "print('OLS Comparison')\n",
    "print(theta_ols_skl,theta_ols_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Comparison\n",
      "[[ 0.4950495   0.29088508 -0.16219036]] [[ 0.49411538]\n",
      " [ 0.28980069]\n",
      " [-0.19545357]]\n"
     ]
    }
   ],
   "source": [
    "#rls l2 penalty\n",
    "#our linear algebra is slightly different from sklearn\n",
    "clf=sklearn.linear_model.Ridge(fit_intercept=False,solver='svd')\n",
    "clf.fit(X,Y)\n",
    "\n",
    "theta_l2_skl=clf.coef_\n",
    "theta_l2_self=ridge(X,Y,lambda_=1)\n",
    "\n",
    "print('Ridge Comparison')\n",
    "print(theta_l2_skl,theta_l2_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 iterations\n",
      "Lasso Comparison\n",
      "[ 7.10542736e-17  1.09742072e-01 -0.00000000e+00] [[ 0.5       ]\n",
      " [ 0.04336246]\n",
      " [-0.00304624]]\n"
     ]
    }
   ],
   "source": [
    "#rls l1 penalty\n",
    "#our implementation is very different from sklearn\n",
    "#it seems lasso is not doing so well compared to ridge\n",
    "clf=sklearn.linear_model.Lasso(fit_intercept=False,\n",
    "                              max_iter=1000,\n",
    "                             tol=0.0001,\n",
    "                              alpha=0.5)\n",
    "clf.fit(X,Y)\n",
    "\n",
    "theta_l1_skl=clf.coef_\n",
    "theta_l1_self=lasso(X,Y,lambda_=0.5,max_itr=1000,\n",
    "                   tol=0.0001,diagnosis=True)\n",
    "\n",
    "print('Lasso Comparison')\n",
    "print(theta_l1_skl,theta_l1_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2 iterations\n",
      "Elastic Net Comparison\n",
      "[ 4.73695157e-17  9.28284491e-02 -0.00000000e+00] [[ 0.5       ]\n",
      " [ 0.02890831]\n",
      " [-0.00203083]]\n"
     ]
    }
   ],
   "source": [
    "#rls l1+l2 penalty\n",
    "#our implementation is very different from sklearn\n",
    "#severely affected by l1 penalty\n",
    "clf=sklearn.linear_model.ElasticNet(fit_intercept=False,\n",
    "                                   alpha=1,l1_ratio=0.5)\n",
    "clf.fit(X,Y)\n",
    "\n",
    "theta_en_skl=clf.coef_\n",
    "theta_en_self=elastic_net(X,Y,alpha=0.5,\n",
    "                          lambda_=1,max_itr=1000,\n",
    "                        tol=0.0001,diagnosis=True)\n",
    "\n",
    "print('Elastic Net Comparison')\n",
    "print(theta_en_skl,theta_en_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute sum of squared errors\n",
    "sse_ols_self=((X@theta_ols_self-Y).T@(X@theta_ols_self-Y)).ravel()[0]\n",
    "sse_l2_self=((X@theta_l2_self-Y).T@(X@theta_l2_self-Y)).ravel()[0]\n",
    "sse_l1_self=((X@theta_l1_self-Y).T@(X@theta_l1_self-Y)).ravel()[0]\n",
    "sse_en_self=((X@theta_en_self-Y).T@(X@theta_en_self-Y)).ravel()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of squared errors from OLS\n",
      " 0.9633040783278697\n",
      "Sum of squared errors from Ridge\n",
      " 0.9833737003537734\n",
      "Sum of squared errors from Lasso\n",
      " 18.544917363017788\n",
      "Sum of squared errors from Elastic Net\n",
      " 20.581900325351896\n"
     ]
    }
   ],
   "source": [
    "#bias variance tradeoff\n",
    "print('Sum of squared errors from OLS\\n',sse_ols_self)\n",
    "print('Sum of squared errors from Ridge\\n',sse_l2_self)\n",
    "print('Sum of squared errors from Lasso\\n',sse_l1_self)\n",
    "print('Sum of squared errors from Elastic Net\\n',sse_en_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast accuracy of OLS 1.0\n",
      "Forecast accuracy of Ridge 1.0\n",
      "Forecast accuracy of Lasso 1.0\n",
      "Forecast accuracy of Elastic Net 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Forecast accuracy of OLS',\n",
    "      len(Y[np.round(X@theta_ols_self,0)==Y])/len(Y))\n",
    "print('Forecast accuracy of Ridge',\n",
    "      len(Y[np.round(X@theta_l2_self,0)==Y])/len(Y))\n",
    "print('Forecast accuracy of Lasso',\n",
    "      len(Y[np.round(X@theta_l1_self,0)==Y])/len(Y))\n",
    "print('Forecast accuracy of Elastic Net',\n",
    "      len(Y[np.round(X@theta_en_self,0)==Y])/len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Choice of Î»\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Unfortunately, there is no smart way to pick the optimal Î» for L1 or L2 penalty. Like any other hyperparameter tuning in machine learning, the optima can only be obtained via brute force. And the result is always arbitrary. Traditionally, there are three different methods. Multiple methods are recommended to use for comparison.\n",
    "\n",
    "* Cross Validation\n",
    "* Akaike Information Criterion\n",
    "* Bayesian Information Criterion\n",
    "\n",
    "You can check this example of Lasso Î» tuning from sklearn.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html\n",
    "\n",
    "Please note that this script computes conventional AIC and BIC where the log likelihood is based upon model error $\\epsilon=(y-x\\theta)$. Albeit AIC-Lasso shrinkage and BIC-Lasso shrinkage is a more common approach for Lasso (Zou, Hastie and Tibshirani, 2007). Check the link below for the original paper.\n",
    "\n",
    "https://www.stanford.edu/~hastie/Papers/dflasso.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our k fold cross validation is purely based upon sum of squared error\n",
    "#as we shuffle the original dataset\n",
    "#the result could be different from each run\n",
    "#in sklearn,this can be done via\n",
    "#sklearn.linear_model.LassoCV and sklearn.linear_model.ElasticNetCV\n",
    "def kfold_cross_validation(X,Y,method,\n",
    "                           n_splits=10,**kwargs):\n",
    "\n",
    "    #k fold\n",
    "    kf=sklearn.model_selection.KFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "    arr=[]\n",
    "    for train_index,test_index in kf.split(X,Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        #regression\n",
    "        theta=method(X_train,Y_train,**kwargs)\n",
    "        sse=((X_test@theta-Y_test).T@(X_test@theta-Y_test)).ravel()[0]\n",
    "        arr.append(sse)\n",
    "            \n",
    "    return np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î» for Ridge via CV\n",
      "0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "#l2 cv\n",
    "arr=[]\n",
    "for i in np.arange(0,1,0.1):\n",
    "    if i==0:\n",
    "        continue\n",
    "    arr.append(kfold_cross_validation(X,Y,ridge,lambda_=i))\n",
    "    \n",
    "print('optimal Î» for Ridge via CV')\n",
    "print(np.arange(0,1,0.1)[1:][arr.index(min(arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î» for Lasso via CV\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "#l1 cv\n",
    "arr=[]\n",
    "for i in np.arange(0,1,0.1):\n",
    "    if i==0:\n",
    "        continue\n",
    "    arr.append(kfold_cross_validation(X,Y,lasso,lambda_=i))\n",
    "    \n",
    "print('optimal Î» for Lasso via CV')\n",
    "print(np.arange(0,1,0.1)[1:][arr.index(min(arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î»,Î± for Elastic Net via CV\n",
      "(0.1, 0.1)\n"
     ]
    }
   ],
   "source": [
    "#l1+l2 cv\n",
    "D={}\n",
    "for i in np.arange(0,1,0.1):\n",
    "    for j in np.arange(0,1,0.1):\n",
    "        if i==0 or j==0:\n",
    "            continue\n",
    "        D[(i,j)]=kfold_cross_validation(X,Y,elastic_net,lambda_=i,alpha=j)\n",
    "\n",
    "print('optimal Î»,Î± for Elastic Net via CV')\n",
    "print(sorted(D.items(),key=lambda x:x[1])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#details of aic can be found in the following link\n",
    "# http://www.modelselection.org/aic/\n",
    "#in sklearn,this can be done via\n",
    "#sklearn.linear_model.LassoLarsIC(criterion='aic')\n",
    "def compute_aic(X,Y,method,**kwargs):\n",
    "\n",
    "    #regression\n",
    "    theta=method(X,Y,**kwargs)\n",
    "    epsilon=Y-X@theta\n",
    "    \n",
    "    #compute log likelihood\n",
    "    miu=epsilon.mean(axis=0)\n",
    "    sigma=epsilon.std(axis=0)\n",
    "    log_likelihood=np.log(\n",
    "        scipy.stats.multivariate_normal(\n",
    "            mean=miu,cov=sigma).pdf(epsilon)).sum()\n",
    "    \n",
    "    return -2*log_likelihood+X.shape[1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î» for Ridge via AIC\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "#l2 aic\n",
    "arr=[]\n",
    "for i in np.arange(0,1,0.1):\n",
    "    if i==0:\n",
    "        continue\n",
    "    arr.append(compute_aic(X,Y,ridge,lambda_=i))\n",
    "    \n",
    "print('optimal Î» for Ridge via AIC')\n",
    "print(np.arange(0,1,0.1)[1:][arr.index(min(arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î» for Lasso via AIC\n",
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "#l1 aic\n",
    "arr=[]\n",
    "for i in np.arange(0,1,0.1):\n",
    "    if i==0:\n",
    "        continue\n",
    "    arr.append(compute_aic(X,Y,lasso,lambda_=i))\n",
    "    \n",
    "print('optimal Î» for Lasso via AIC')\n",
    "print(np.arange(0,1,0.1)[1:][arr.index(min(arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î»,Î± for Elastic Net via AIC\n",
      "(0.1, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "#l1+l2 aic\n",
    "D={}\n",
    "for i in np.arange(0,1,0.1):\n",
    "    for j in np.arange(0,1,0.1):\n",
    "        if i==0 or j==0:\n",
    "            continue\n",
    "        D[(i,j)]=compute_aic(X,Y,elastic_net,lambda_=i,alpha=j)\n",
    "\n",
    "print('optimal Î»,Î± for Elastic Net via AIC')\n",
    "print(sorted(D.items(),key=lambda x:x[1])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#details of bic can be found in the following link\n",
    "# http://www.modelselection.org/bic/\n",
    "#in sklearn,this can be done via\n",
    "#sklearn.linear_model.LassoLarsIC(criterion='bic')\n",
    "def compute_bic(X,Y,method,**kwargs):\n",
    "\n",
    "    #regression\n",
    "    theta=method(X,Y,**kwargs)\n",
    "    epsilon=Y-X@theta\n",
    "    miu=epsilon.mean(axis=0)\n",
    "    sigma=epsilon.std(axis=0)\n",
    "    \n",
    "    #compute log likelihood\n",
    "    log_likelihood=np.log(\n",
    "        scipy.stats.multivariate_normal(\n",
    "            mean=miu,cov=sigma).pdf(epsilon)).sum()\n",
    "    \n",
    "    return -2*log_likelihood+X.shape[1]*np.log(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î» for Ridge via BIC\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "#l2 bic\n",
    "arr=[]\n",
    "for i in np.arange(0,1,0.1):\n",
    "    if i==0:\n",
    "        continue\n",
    "    arr.append(compute_bic(X,Y,ridge,lambda_=i))\n",
    "    \n",
    "print('optimal Î» for Ridge via BIC')\n",
    "print(np.arange(0,1,0.1)[1:][arr.index(min(arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î» for Lasso via BIC\n",
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "#l1 bic\n",
    "arr=[]\n",
    "for i in np.arange(0,1,0.1):\n",
    "    if i==0:\n",
    "        continue\n",
    "    arr.append(compute_bic(X,Y,lasso,lambda_=i))\n",
    "    \n",
    "print('optimal Î» for Lasso via BIC')\n",
    "print(np.arange(0,1,0.1)[1:][arr.index(min(arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Î»,Î± for Elastic Net via BIC\n",
      "(0.1, 0.1)\n"
     ]
    }
   ],
   "source": [
    "#l1+l2 bic\n",
    "D={}\n",
    "for i in np.arange(0,1,0.1):\n",
    "    for j in np.arange(0,1,0.1):\n",
    "        if i==0 or j==0:\n",
    "            continue\n",
    "        D[(i,j)]=compute_bic(X,Y,elastic_net,lambda_=i,alpha=j)\n",
    "\n",
    "print('optimal Î»,Î± for Elastic Net via BIC')\n",
    "print(sorted(D.items(),key=lambda x:x[1])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "156px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
